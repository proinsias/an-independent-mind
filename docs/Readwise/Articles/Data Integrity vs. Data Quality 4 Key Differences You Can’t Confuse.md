# Data Integrity vs. Data Quality: 4 Key Differences You Can’t Confuse

![rw-book-cover](https://www.montecarlodata.com/wp-content/uploads/2023/06/Data-integrity-vs-data-quality-150x150.jpg)

## Metadata
- Author: [[Michael Segner]]
- Full Title: Data Integrity vs. Data Quality: 4 Key Differences You Can’t Confuse
- Category: #articles
- URL: https://www.montecarlodata.com/blog-data-integrity-vs-data-quality/

## Highlights
- Data quality, on the other hand, is about how well the data serves its intended purpose. This involves [several elements](https://www.montecarlodata.com/blog-how-to-fix-your-data-quality-problem/), including accuracy, completeness, consistency, timeliness, and relevance. ([View Highlight](https://read.readwise.io/read/01h33eq3j13a9564nxk9sq0dep))
- If we extend the mail carrier analogy, data quality doesn’t just mean the letter gets to point B, it goes a few steps further. It checks that it’s the right letter, it’s clear and understandable, arrives exactly when it’s needed, and follows a consistent format. ([View Highlight](https://read.readwise.io/read/01h33eqancdp5t8amnb1sx3agc))
- You can have data quality, without data integrity. For example, data is frequently transformed, manipulated, and combined with other data to make it more suitable for its intended purpose. ([View Highlight](https://read.readwise.io/read/01h33eqftez3pdrpd3nshk14y8))
- In short, data integrity focuses on the preservation and protection of data’s original state, while data quality is concerned with how suitable and effective that data is for its intended purposes. ([View Highlight](https://read.readwise.io/read/01h33eqha1f6zrjbvtrns9zzvf))
- Data integrity refers to the accuracy and consistency of data over its lifecycle. When data has integrity, it means it wasn’t altered during storage, retrieval, or processing **without authorization.** It’s like making sure a letter gets from point A to point B without any of its content being changed. ([View Highlight](https://read.readwise.io/read/01h33epwfxy26sx0e8kwtm5esz))
- The impact of data integrity is profound, particularly when it comes to governance and compliance. If an organization loses regulated data, or it is corrupted while in their care, the fines can be severe. ([View Highlight](https://read.readwise.io/read/01h33er7gtqtnc6ccdc5sf02tz))
- Data quality, in contrast, impacts the usability and effectiveness of data. If the data isn’t accurate, complete, timely, and relevant, you run the risk of making decisions based on “false truths.” Poor data quality can lead to misguided decisions, inefficient operations, and missed opportunities. ([View Highlight](https://read.readwise.io/read/01h33erd39vkyxsqzdnb8g8n35))
- Data integrity is crucial in fields where the preservation of original data is paramount. Consider the healthcare industry, where patient records must remain unaltered to ensure correct diagnoses and treatments. Even if an incorrect diagnoses was made, you would still want that to remain on the record (along with the correct updated diagnoses) so a medical professional could understand the full history, and context of a patient’s journey. ([View Highlight](https://read.readwise.io/read/01h33ersdjh78qnnm1wamxxx1y))
- Maintaining data integrity involves a mix of physical security measures, user access controls, and system checks: ([View Highlight](https://read.readwise.io/read/01h33esg5g4x1wsfrjawtdweer))
- The field of data engineering is rife with examples of common techniques and best practices designed to maintain data integrity. For example, primary keys are used within SQL databases to uniquely identify each row in a table. [Data vault](https://www.montecarlodata.com/blog-data-vault-architecture-data-quality/) is a data modeling strategy that preserves 100% of the data–rather than modifying rows, a new row with the correction would be inserted into the table. ([View Highlight](https://read.readwise.io/read/01h33esxtppg8990d0zrdrbckd))
- Data teams often use [quality checks](https://www.montecarlodata.com/blog-data-quality-testing/), unit tests and validation rules can be used to check the data at the point of entry and at each transformation stage in your data pipeline to prevent incorrect or inconsistent data from being added to the system. [This strategy can lead to scaling challenges](https://www.montecarlodata.com/blog-data-observability-vs-data-testing-everything-you-need-to-know/) as updating and maintaining this code across tables can be tedious, and will only catch the errors you anticipate. ([View Highlight](https://read.readwise.io/read/01h33evc5xjzv63cskw7kqdrty))
- That’s why data teams leveraging a [modern data stack](https://www.montecarlodata.com/blog-the-future-of-the-modern-data-stack/) are starting to widely adopt [data observability](https://www.montecarlodata.com/blog-what-is-data-observability/) platforms like [Monte Carlo](https://www.montecarlodata.com/). Data observability automatically monitors both your data pipelines and the data running through them by leveraging machine learning monitors to alert you when there is anomalous behavior without the need to set any manual rules or thresholds (although you can supplement this coverage with custom monitors as well). ([View Highlight](https://read.readwise.io/read/01h33evdmq0b4hngqv9rz0cfnb))
