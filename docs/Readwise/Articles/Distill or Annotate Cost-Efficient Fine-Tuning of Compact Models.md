---
date: 2023-06-06 14:57
last_modified_at: 2023-06-06 14:57
---
# Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models

![rw-book-cover](https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)

## Metadata
- Author: [[Alan Ritter]]
- Full Title: Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models
- Category: #articles
- URL: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2305.01645%3Futm_source=tldrai/1/010001887c442898-8f90d012-2167-44d2-a424-aaeb0beb18b5-000000/X2twRT2TPW_5okPPCWMwnHPXHNp7JZXOOn9Sn6ORq_8=303

## Highlights
- Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner who needs a compact model might also choose to simply allocate an available budget to hire annotators and manually label additional fine-tuning data ([View Highlight](https://read.readwise.io/read/01h20m2w0f9dccj4mp8ghh9a7x))
- we find that distilling from T5-XXL (11B) to T5-Small (60M) leads to almost always a cost-efficient option compared to annotating more data to directly train a compact model (T5-Small (60M)). ([View Highlight](https://read.readwise.io/read/01h20m39pv8p0e4dzwtqe2twhq))
# Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models

![rw-book-cover](https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)

## Metadata
- Author: [[Alan Ritter]]
- Full Title: Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models
- Category: #articles
- URL: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2305.01645%3Futm_source=tldrai/1/010001887c442898-8f90d012-2167-44d2-a424-aaeb0beb18b5-000000/X2twRT2TPW_5okPPCWMwnHPXHNp7JZXOOn9Sn6ORq_8=303

## Highlights
- Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner who needs a compact model might also choose to simply allocate an available budget to hire annotators and manually label additional fine-tuning data ([View Highlight](https://read.readwise.io/read/01h20m2w0f9dccj4mp8ghh9a7x))
- we find that distilling from T5-XXL (11B) to T5-Small (60M) leads to almost always a cost-efficient option compared to annotating more data to directly train a compact model (T5-Small (60M)). ([View Highlight](https://read.readwise.io/read/01h20m39pv8p0e4dzwtqe2twhq))
