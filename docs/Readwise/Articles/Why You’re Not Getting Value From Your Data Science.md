# Why You’re Not Getting Value From Your Data Science

![rw-book-cover](https://hbr.org/resources/images/article_assets/2016/12/dec16-07-674734177.jpg)

## Metadata
- Author: [[Kalyan Veeramachaneni]]
- Full Title: Why You’re Not Getting Value From Your Data Science
- Category: #articles
- URL: https://hbr.org/2016/12/why-youre-not-getting-value-from-your-data-science

## Highlights
- In its rawest form, even clean data is too overwhelming and complex to be understood at first glance, even by experts. It has too many tables and fields and is often collected at a very high granularity (for example, online clickstreams generate new data with every click, and sensor data is collected at 125 observations per second). Machine learning experts are used to working with data that’s already been aggregated into useful variables ([View Highlight](https://read.readwise.io/read/01h3d2mqmpwrmk1fc25n3gpfss))
- while business experts are coming up with problems, machine learning experts cannot always keep up. ([View Highlight](https://read.readwise.io/read/01h3d2n3tqgrww5pzs8wswygse))
- If companies want to get value from their data, they need to focus on accelerating human understanding of data, scaling the number of modeling questions they can ask of that data in a short amount of time, and assessing their implications. In our work with companies, we ultimately decided that creating true impact via machine learning will come from a focus on four principles:
  **Stick with simple models:** We decided that simple models, like logistic regression or those based on [random forests](https://en.wikipedia.org/wiki/Random_forest) or decision trees, are sufficient for the problems at hand. The focus should instead be on reducing the time between the data acquisition and the development of the first simple predictive model.
  **Explore more problems:** Data scientists need the ability to rapidly define and explore multiple prediction problems, quickly and easily. Instead of exploring one business problem with an incredibly sophisticated machine learning model, companies should be exploring dozens, building a simple predictive model for each one and assessing their value proposition.
  **Learn from a sample of data—not all the data:** Instead of focusing on how to apply distributed computing to allow any individual processing module to handle big data, invest in techniques that will enable the derivations of similar conclusions from a data subsample. By circumventing the use of massive computing resources, they will enable the exploration of more hypotheses.
  **Focus on automation:** To achieve both *reduced time to first model* and *increased rate of exploration*, companies must automate processes that are normally done manually. Over and over across different data problems, we found ourselves applying similar data processing techniques, whether it was to transform the data into useful aggregates, or to prepare data for predictive modeling—it’s time to streamline these, and to develop algorithms and build software systems that do them automatically. ([View Highlight](https://read.readwise.io/read/01h3d2ndfefc7h8sh7vv8y6nkj))
